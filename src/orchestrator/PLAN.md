# Implementation Plan: Skill-Based Subagent Feature (Sisyphus v2 Evolution)

## Overview

This plan documents the evolution of the orchestrator module into **Sisyphus v2**, a high-fidelity orchestration system that merges the background lifecycle of **oh-my-opencode (OMO)** with the "Continuity Ledger" pattern from **Continuous Claude v2 (CCv2)**.

**Status:** Phases 1-4 Complete âœ… | Phase 6: Sisyphus v2 Evolution (Active) ğŸš§

The system leverages the **Hybrid Delegator Pattern** to spawn skill-defined subagents that operate as independent **Actors** on a durable coordination bus.

---

## 1. System Design: The Durable Actor Pattern

Sisyphus v2 is designed as a **non-invasive sidecar** that bridges the OpenCode SDK with advanced orchestration patterns. It treats the context window as a volatile cache and the file system + durable streams as the source of truth.

### 1.1 Architectural Visualization: Durable Stream

The Durable Stream is the immutable backbone. It is a log-structured record of every decision and state transition.

```text
DURABLE STREAM (PGLite / SQL Event Log)
+-----------------------------------------------------------------------+
|                           EVENT_LOG TABLE                             |
+-----------------------------------------------------------------------+
| Offset: 0 | Offset: 1 | Offset: 2 | Offset: 3 |       ...             |
+-----------+-----------+-----------+-----------+-----------------------+
| TASK_INIT | PLAN_INIT | FILE_EDIT | TEST_PASS | [Next Write Head] ->  |
+-----------+-----------+-----------+-----------+-----------------------+
      â”‚           â”‚               â”‚           â”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              MATERIALIZED VIEW (Ledger)
       [ Current Status: IMPLEMENTING | Phase: 3/4 ]
       [ SISYPHUS_LEDGER.md is the human-readable Projection ]
```

### 1.2 Architectural Visualization: Actor Model

Agents are independent Actors that communicate exclusively via the Durable Stream.

```text
      [Sisyphus Kernel]               [Executor Actor (Sub-agent)]
            â”‚                                     â”‚
            â”‚ 1. skill_agent(executor)            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
            â”‚                                     â”‚ 2. swarmmail_init()
            â”‚ 3. swarmmail_send(task_payload)     â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
            â”‚                                     â”‚ 4. processInbox()
            â”‚                                     â”‚ (Reads from stream)
            â”‚                                     â”‚
            â”‚           5. STATUS_UPDATE          â”‚
            â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚      (Appended to Durable Log)      â”‚
```

---

## 2. Technical Implementation Patterns (TypeScript)

### 2.1 The Durable Stream Primitive

This pattern ensures that state transitions are immutable and re-hydratable.

```typescript
// src/orchestrator/sisyphus/stream.ts
export interface SisyphusEvent<T = any> {
  offset: number;
  type: string;
  payload: T;
  timestamp: number;
}

export class DurableSisyphusStream {
  // In production, this wraps the PGLite 'events' table
  async append<T>(type: string, payload: T): Promise<number> {
    const offset = await db.insert('events').values({ type, payload });
    return offset;
  }

  async readFrom(offset: number): Promise<SisyphusEvent[]> {
    return db.select().from('events').where(gt('offset', offset)).orderBy('offset');
  }
}
```

### 2.1.1 PGLite Schema Definition (Event Sourcing)

The Durable Stream is backed by a libSQL/SQLite database following swarm-tools event sourcing patterns. This schema is append-only and optimized for replay.

**SQL Schema: `sisyphus_events` Table**

```sql
-- Primary event log table - append-only, immutable
CREATE TABLE sisyphus_events (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  sequence INTEGER NOT NULL UNIQUE,  -- Monotonic offset for ordering
  type TEXT NOT NULL,                 -- Event type discriminator
  payload TEXT NOT NULL,              -- JSON-serialized payload
  project_key TEXT NOT NULL,          -- Project identifier (partition key)
  created_at INTEGER NOT NULL DEFAULT (strftime('%s', 'subsec'))
);

-- Index for efficient time-based queries
CREATE INDEX idx_sisyphus_events_project_sequence
  ON sisyphus_events(project_key, sequence);

-- Index for type-based filtering
CREATE INDEX idx_sisyphus_events_type
  ON sisyphus_events(type);

-- Materialized projection tables (derived from events)
CREATE TABLE sisyphus_agents (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  agent_name TEXT NOT NULL UNIQUE,
  status TEXT NOT NULL,               -- 'idle' | 'busy' | 'crashed'
  last_sequence INTEGER DEFAULT 0,    -- Last processed offset
  project_key TEXT NOT NULL,
  created_at INTEGER NOT NULL,
  updated_at INTEGER NOT NULL
);

CREATE TABLE sisyphus_cursors (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  project_key TEXT NOT NULL,
  consumer_id TEXT NOT NULL,         -- Unique consumer identifier
  position INTEGER NOT NULL DEFAULT 0,  -- Last processed sequence
  last_updated INTEGER NOT NULL,
  UNIQUE(project_key, consumer_id)
);
```

**TypeScript Type Definitions**

```typescript
// src/orchestrator/sisyphus/types.ts
export type SisyphusEventType =
  | 'TASK_INIT'
  | 'PLAN_CREATED'
  | 'AGENT_SPAWNED'
  | 'STATUS_UPDATE'
  | 'FILE_EDIT'
  | 'TEST_RESULT'
  | 'VALIDATION_RESULT'
  | 'TASK_COMPLETE'
  | 'TASK_FAILED';

export type SisyphusAgentStatus = 'idle' | 'busy' | 'crashed';

export interface SisyphusEventPayload {
  TASK_INIT: {
    task: string;
    epicId: string;
    initiator: string;
  };
  PLAN_CREATED: {
    planId: string;
    phases: Array<{
      name: string;
      status: 'pending' | 'in_progress' | 'completed' | 'failed';
      files: string[];
    }>;
  };
  AGENT_SPAWNED: {
    agentName: string;
    skill: string;
    beadId: string;
  };
  STATUS_UPDATE: {
    beadId: string;
    status: 'in_progress' | 'completed' | 'failed';
    progressPercent: number;
  };
  FILE_EDIT: {
    filePath: string;
    description: string;
    diff: string;
  };
  TEST_RESULT: {
    testName: string;
    passed: boolean;
    output: string;
  };
  VALIDATION_RESULT: {
    type: 'accept' | 'reject';
    criteria: string[];
    issues?: string[];
  };
  TASK_COMPLETE: {
    epicId: string;
    summary: string;
    filesTouched: string[];
  };
  TASK_FAILED: {
    epicId: string;
    error: string;
    retryCount: number;
  };
}

export interface SisyphusEvent {
  id: number;
  sequence: number;
  type: SisyphusEventType;
  payload: string; // JSON-serialized SisyphusEventPayload[T]
  projectKey: string;
  createdAt: number;
}

// Type-safe event builder
export class SisyphusEventBuilder {
  static create<T extends SisyphusEventType>(
    type: T,
    payload: SisyphusEventPayload[T],
    projectKey: string,
    sequence: number
  ): SisyphusEvent {
    return {
      id: 0, // Assigned by database
      sequence,
      type,
      payload: JSON.stringify(payload),
      projectKey,
      createdAt: Date.now(),
    };
  }

  static parse<T extends SisyphusEventType>(
    event: SisyphusEvent
  ): { type: T; payload: SisyphusEventPayload[T] } {
    return {
      type: event.type as T,
      payload: JSON.parse(event.payload) as SisyphusEventPayload[T],
    };
  }
}
```

**DurableSisyphusStream Implementation**

```typescript
// src/orchestrator/sisyphus/stream.ts
import { drizzle } from 'drizzle-orm/better-sqlite3';
import Database from 'better-sqlite3';
import { sisyphusEvents, sisyphusCursors } from './schema';

export class DurableSisyphusStream {
  private db: ReturnType<typeof drizzle>;

  constructor(private dbPath: string = '~/.config/swarm-tools/sisyphus.db') {
    const sqlite = new Database(dbPath);
    this.db = drizzle(sqlite);
  }

  /**
   * Append a new event to the immutable log.
   * Returns the assigned sequence number (offset).
   */
  async append<T extends SisyphusEventType>(
    type: T,
    payload: SisyphusEventPayload[T],
    projectKey: string
  ): Promise<number> {
    const result = await this.db
      .insert(sisyphusEvents)
      .values({
        type,
        payload: JSON.stringify(payload),
        projectKey,
        createdAt: Date.now(),
      })
      .returning({ sequence: sisyphusEvents.sequence });

    return result[0].sequence;
  }

  /**
   * Read all events from a given offset for a specific project.
   * Used by actors to replay history or consume new messages.
   */
  async readFrom(
    sequence: number,
    projectKey: string,
    limit: number = 100
  ): Promise<SisyphusEvent[]> {
    const events = await this.db
      .select()
      .from(sisyphusEvents)
      .where(and(eq(sisyphusEvents.projectKey, projectKey), gt(sisyphusEvents.sequence, sequence)))
      .orderBy(asc(sisyphusEvents.sequence))
      .limit(limit);

    return events;
  }

  /**
   * Track a consumer's read position (checkpointing).
   * Follows swarm-tools DurableCursor pattern for recovery.
   */
  async saveCursor(consumerId: string, projectKey: string, position: number): Promise<void> {
    await this.db
      .insert(sisyphusCursors)
      .values({
        projectKey,
        consumerId,
        position,
        lastUpdated: Date.now(),
      })
      .onConflictDoUpdate({
        target: [sisyphusCursors.projectKey, sisyphusCursors.consumerId],
        set: {
          position,
          lastUpdated: Date.now(),
        },
      });
  }

  /**
   * Retrieve a consumer's last checkpoint.
   */
  async getCursor(consumerId: string, projectKey: string): Promise<number | null> {
    const result = await this.db
      .select({ position: sisyphusCursors.position })
      .from(sisyphusCursors)
      .where(
        and(eq(sisyphusCursors.projectKey, projectKey), eq(sisyphusCursors.consumerId, consumerId))
      )
      .limit(1);

    return result[0]?.position ?? null;
  }

  /**
   * Reconstruct state by replaying events from the beginning.
   * This enables crash recovery and context clearing.
   */
  async replay<T>(
    projectKey: string,
    reducer: (state: T, event: SisyphusEvent) => T,
    initialState: T
  ): Promise<T> {
    const events = await this.readFrom(-1, projectKey, 10_000);
    return events.reduce(reducer, initialState);
  }
}
```

**Test Requirements (TDD)**

Following swarm-tools patterns, the roadmap must include:

1. **Unit Tests for DurableSisyphusStream**:
   - Test append assigns monotonically increasing sequences
   - Test readFrom returns events after given offset
   - Test cursor persistence across database restarts
   - Test replay reconstructs correct state

2. **Integration Tests with PGLite**:
   - Test concurrent appends don't corrupt sequence ordering
   - Test project-level isolation (events from project A don't leak to project B)
   - Test JSON payload serialization/deserialization roundtrip

3. **Schema Migration Tests**:
   - Verify table creation and index correctness
   - Test foreign key constraints on projection tables

### 2.2 The Sisyphus Actor Base

Every specialized sub-agent (Planner, Validator, Executor) follows the **Actor Model** protocol with durable state persistence.

#### 2.2.1 SisyphusActor Abstract Class

The base class provides the core actor lifecycle: boot, message processing, and checkpointing.

```typescript
// src/orchestrator/sisyphus/actor.ts
import { DurableSisyphusStream } from './stream';
import { SisyphusEvent, SisyphusEventType } from './types';

export type SisyphusActorStatus = 'launching' | 'running' | 'completed' | 'failed';

export abstract class SisyphusActor {
  protected stream: DurableSisyphusStream;
  protected consumerId: string; // Unique actor identifier
  protected projectKey: string;
  protected cursor: number = 0; // Last processed sequence
  protected status: SisyphusActorStatus = 'launching';

  constructor(
    protected agentName: string,
    projectKey: string,
    dbPath?: string
  ) {
    this.consumerId = `${agentName}-${Date.now()}`;
    this.projectKey = projectKey;
    this.stream = new DurableSisyphusStream(dbPath);
  }

  /**
   * Boot sequence: Join the bus, restore state, catch up on history.
   * This is the OMO-compatible lifecycle entry point.
   */
  async boot(): Promise<void> {
    this.status = 'launching';

    // 1. Initialize Swarm Mail (the Radio)
    await swarmmail_init({
      project_path: this.projectKey,
      agent_name: this.agentName,
    });

    // 2. Restore cursor from durable storage
    const savedCursor = await this.stream.getCursor(this.consumerId, this.projectKey);
    if (savedCursor !== null) {
      this.cursor = savedCursor;
    }

    // 3. Catch up on missed history (rehydration)
    await this.processInbox();

    // 4. Transition to running state
    this.status = 'running';
  }

  /**
   * The Continuity Loop: Consumes durable messages from the event stream.
   * Uses DurableCursor pattern for crash recovery and context clearing.
   */
  async processInbox(): Promise<void> {
    while (this.status === 'running') {
      // Read events from last checkpoint position
      const events = await this.stream.readFrom(
        this.cursor,
        this.projectKey,
        100 // Batch size for efficient processing
      );

      if (events.length === 0) {
        // No new events, wait before polling again
        await this.sleep(1000);
        continue;
      }

      // Process each event in order
      for (const event of events) {
        await this.onMessage(event);
        this.cursor = event.sequence + 1;

        // Checkpoint after each event for crash recovery
        await this.stream.saveCursor(this.consumerId, this.projectKey, this.cursor);
      }
    }
  }

  /**
   * Abstract method: Subclasses implement event handling logic.
   */
  protected abstract onMessage(event: SisyphusEvent): Promise<void>;

  /**
   * Shutdown: Clean up and mark completion.
   */
  async shutdown(success: boolean = true): Promise<void> {
    this.status = success ? 'completed' : 'failed';
    await this.stream.saveCursor(this.consumerId, this.projectKey, this.cursor);
    // Swarm Mail session cleanup is automatic
  }

  /**
   * Utility: Sleep for specified milliseconds.
   */
  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  /**
   * Emit event to the durable stream.
   * Allows actors to coordinate via the shared event log.
   */
  protected async emit<T extends SisyphusEventType>(type: T, payload: any): Promise<number> {
    return await this.stream.append(type, payload, this.projectKey);
  }
}
```

#### 2.2.2 Continuity Loop Logic

The Continuity Loop is the core resilience mechanism that enables actors to survive crashes, restarts, and context clearing.

**How It Works:**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONTINUITY LOOP                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. RESTORE CURSOR:                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ Get Cursor   â”‚â”€â”€â”€â”€â”                                   â”‚
â”‚     â”‚ from DB      â”‚    â”‚                                   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                   â”‚
â”‚            â”‚            â”‚                                   â”‚
â”‚            â–¼            â–¼                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚     â”‚  cursor =    â”‚ <â”€â”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚  last_seq    â”‚   â”‚                              â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                              â”‚    â”‚
â”‚           â”‚            â”‚                              â”‚    â”‚
â”‚           â–¼            â”‚                              â”‚    â”‚
â”‚  2. READ EVENTS:      â”‚                              â”‚    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                              â”‚    â”‚
â”‚     â”‚ events =     â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚ readFrom(    â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚  cursor)     â”‚   â”‚                              â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                              â”‚    â”‚
â”‚           â”‚            â”‚                              â”‚    â”‚
â”‚           â–¼            â”‚                              â”‚    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                              â”‚    â”‚
â”‚     â”‚ if empty?    â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚   wait 1s   â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚   loop again â”‚   â”‚                              â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                              â”‚    â”‚
â”‚           â”‚            â”‚                              â”‚    â”‚
â”‚           â–¼            â”‚                              â”‚    â”‚
â”‚  3. PROCESS EVENTS:    â”‚                              â”‚    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                              â”‚    â”‚
â”‚     â”‚ for event in â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚   events:    â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚   onMessage  â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚   cursor++   â”‚   â”‚                              â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                              â”‚    â”‚
â”‚           â”‚            â”‚                              â”‚    â”‚
â”‚           â–¼            â”‚                              â”‚    â”‚
â”‚  4. CHECKPOINT:       â”‚                              â”‚    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                              â”‚    â”‚
â”‚     â”‚ saveCursor(  â”‚   â”‚                              â”‚    â”‚
â”‚     â”‚  cursor)     â”‚   â”‚                              â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                              â”‚    â”‚
â”‚                         â”‚                              â”‚    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚    â”‚
â”‚              â”‚                                         â”‚    â”‚
â”‚              â–¼                                         â”‚    â”‚
â”‚     [SYSTEM CRASH] â”€â”€â”€â”€â”€â”€â”€â–º  [RESTART] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Properties:**

| Property                | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| **Offset-Based**        | Uses sequence numbers, never timestamps (deterministic replay) |
| **Exactly-Once**        | Checkpointing prevents duplicate processing                    |
| **Crash-Safe**          | Cursor persisted before each event, state survives restart     |
| **Context-Independent** | Can resume from any point without conversation history         |

**Rehydration Example:**

```typescript
// Scenario: Actor crashes after processing 5 events
// Cursor saved: 5

// On restart:
await actor.boot();
// 1. Load cursor: cursor = 5
// 2. Read from offset 5: gets events 6, 7, 8...
// 3. Process only new events, never re-processes 1-5
```

#### 2.2.3 OMO-Compatible Background Lifecycle

The actor lifecycle maps directly to OMO (oh-my-opencode) background task states:

```typescript
// Lifecycle state machine
enum OMOBackgroundState {
  LAUNCHING = 'launching',  // Actor.boot() called
  RUNNING = 'running',      // Processing inbox
  COMPLETED = 'completed',  // Actor.shutdown(true)
  FAILED = 'failed'          // Actor.shutdown(false)
}

// State transitions
LAUNCHING â”€â”€â–º RUNNING â”€â”€â–º COMPLETED
                 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â–º FAILED
```

**OMO Integration Points:**

| OMO Hook                   | SisyphusActor Method | Purpose                    |
| -------------------------- | -------------------- | -------------------------- |
| `background_task.spawn()`  | `boot()`             | Launch actor in background |
| `background_task.status()` | `status` getter      | Query current state        |
| `background_task.kill()`   | `shutdown(false)`    | Force termination          |
| `idle()` hook              | Continuity loop      | Auto-resume after crash    |

**TUI Status Display:**

```typescript
// OMO can query actor status for TUI display
const status = await swarmmail_inbox({ status_only: true });
// TUI renders: [RUNNING] Executor Actor - Processing offset 127
```

#### 2.2.4 Actor Resilience

**Handling Restarts:**

```typescript
// Scenario 1: Graceful restart (user requested)
await actor.shutdown(true);
await actor.boot(); // Resumes from last checkpoint

// Scenario 2: Crash recovery (unexpected termination)
// On next boot():
// - Cursor restored from DB
// - Missed events replayed automatically
// - State re-hydrated to last known good state
```

**Maintaining Cursor State:**

Cursor persistence follows swarm-tools DurableCursor pattern:

1. **Atomic Writes**: Checkpointed before processing completes
2. **Idempotent Processing**: Replaying same event produces same result
3. **Consistent Ordering**: Events processed in strict sequence order
4. **Recoverable**: Can always restore to any previous checkpoint

**Failure Mode Handling:**

| Failure Type            | Detection                      | Recovery                        |
| ----------------------- | ------------------------------ | ------------------------------- |
| Process crash           | Checkpoint exists, inbox empty | Resume from cursor              |
| Database corruption     | Cursor read fails              | Reset cursor to 0, replay all   |
| Message deserialization | Parse error                    | Log error, skip event, continue |
| Stuck actor             | No progress for 5 min          | Timeout, force restart          |

**Cursor Integrity:**

```typescript
// Ensure cursor never goes backward (prevent state corruption)
async saveCursor(position: number): Promise<void> {
  if (position < this.cursor) {
    throw new Error(
      `Cursor regression: ${position} < ${this.cursor}. ` +
      'This indicates a logic error in event processing.'
    );
  }
  this.cursor = position;
  await this.stream.saveCursor(this.consumerId, this.projectKey, position);
}
```

#### 2.2.5 Test Requirements (TDD)

Following swarm-tools patterns, the actor specification must include:

**Unit Tests for SisyphusActor:**

1. **Boot Sequence**:
   - Test that boot() initializes Swarm Mail
   - Test cursor restoration from database
   - Test status transitions: launching â†’ running

2. **Continuity Loop**:
   - Test processInbox() reads from correct offset
   - Test checkpointing after each event
   - Test polling when inbox is empty
   - Test loop termination on shutdown()

3. **Crash Recovery**:
   - Test that restart skips already-processed events
   - Test cursor persistence across actor instances
   - Test state re-hydration from event stream

4. **OMO Lifecycle Integration**:
   - Test that status maps to OMO background states
   - Test that shutdown() completes gracefully
   - Test that failed shutdown sets correct status

**Integration Tests with Durable Stream:**

1. **Multi-Actor Coordination**:
   - Test two actors processing same event stream
   - Test event emission from one actor triggers another
   - Test cursor isolation between actors

2. **Failure Scenarios**:
   - Test actor crash mid-event processing
   - Test database connection loss and recovery
   - Test message deserialization failure

**Schema Tests:**

1. **Cursor Table**:
   - Test unique constraint on (project_key, consumer_id)
   - Test upsert behavior (insert vs update)
   - Test cursor position monotonicity

**Example Test Structure:**

```typescript
describe('SisyphusActor', () => {
  it('should restore cursor on boot', async () => {
    const actor = new TestActor('test-actor', projectKey, testDbPath);
    await actor.boot();

    // Simulate processing 3 events
    await actor.emit('TEST_EVENT', { seq: 1 });
    await actor.emit('TEST_EVENT', { seq: 2 });
    await actor.emit('TEST_EVENT', { seq: 3 });

    // Shutdown and restart
    await actor.shutdown(true);
    const actor2 = new TestActor('test-actor', projectKey, testDbPath);
    await actor2.boot();

    // Verify cursor restored (should not re-process events 1-3)
    expect(actor2.cursor).toBe(3);
  });

  it('should handle crash recovery', async () => {
    const actor = new TestActor('test-actor', projectKey, testDbPath);
    await actor.boot();

    // Process events
    await actor.processInbox();

    // Simulate crash (process terminates without shutdown)
    // Cursor saved at position 42

    // Restart
    const actor2 = new TestActor('test-actor', projectKey, testDbPath);
    await actor2.boot();

    // Should resume from position 43, not 0
    expect(actor2.cursor).toBe(42);
  });
});
```

---

## 3. Persistence & Handoff Primitives (Ledger)

Sisyphus v2 implements the **Continuity Ledger** pattern from CCv2 (Continuous Claude v2) to maintain state across context wipes and process restarts. The ledger serves as the human-readable materialized view of the durable event stream.

### 3.1 SISYPHUS_LEDGER.md Format

The ledger is a markdown file stored at `.sisyphus/SISYPHUS_LEDGER.md` that provides a complete snapshot of current work state. It is updated atomically on every state transition.

**File Structure:**

```markdown
# Sisyphus Ledger

## Goal

[Human-readable description of the current task/epic]

## Constraints

- [Constraint 1: Technical limitation or requirement]
- [Constraint 2: Deadline or resource constraint]
- [Constraint 3: Integration requirement]

## State

**Current Phase**: [phase name]
**Status**: [in_progress | blocked | completed | failed]
**Last Updated**: [ISO timestamp]
**Stream Offset**: [N]

## Decisions

| Offset | Decision Type  | Rationale                                         | Actor    |
| ------ | -------------- | ------------------------------------------------- | -------- |
| 12     | ARCHITECTURE   | Use PGLite for durable storage                    | planner  |
| 23     | IMPLEMENTATION | Follow swarm-tools patterns for cursor management | executor |

## Working Set

| File                       | Status    | Last Modified        | Offset |
| -------------------------- | --------- | -------------------- | ------ |
| src/orchestrator/stream.ts | editing   | 2025-12-29T14:30:00Z | 45     |
| src/orchestrator/actor.ts  | pending   | -                    | -      |
| test/stream.test.ts        | completed | 2025-12-29T14:25:00Z | 38     |

## Handoff Files

- `.sisyphus/HANDOFF_precompact_2025-12-29T14:45:00Z.md` - Last precompact snapshot
```

**Ledger Update Pattern:**

```typescript
// src/orchestrator/sisyphus/ledger.ts
export class SisyphusLedger {
  constructor(private projectPath: string) {}

  /**
   * Atomically update the ledger with current state.
   * Called after every state transition.
   */
  async update(state: LedgerState): Promise<void> {
    const ledgerPath = path.join(this.projectPath, '.sisyphus', 'SISYPHUS_LEDGER.md');
    const content = this.renderLedger(state);

    // Atomic write: write to temp file, then rename
    const tempPath = `${ledgerPath}.tmp`;
    await fs.writeFile(tempPath, content, 'utf-8');
    await fs.rename(tempPath, ledgerPath);
  }

  private renderLedger(state: LedgerState): string {
    return `# Sisyphus Ledger

## Goal
${state.goal}

## Constraints
${state.constraints.map((c) => `- ${c}`).join('\n')}

## State
**Current Phase**: ${state.currentPhase}
**Status**: ${state.status}
**Last Updated**: ${new Date(state.lastUpdated).toISOString()}
**Stream Offset**: ${state.streamOffset}

## Decisions
| Offset | Decision Type | Rationale | Actor |
|--------|---------------|-----------|-------|
${state.decisions.map((d) => `| ${d.offset} | ${d.type} | ${d.rationale} | ${d.actor} |`).join('\n')}

## Working Set
| File | Status | Last Modified | Offset |
|------|--------|---------------|--------|
${state.workingSet.map((f) => `| ${f.file} | ${f.status} | ${new Date(f.lastModified).toISOString()} | ${f.offset} |`).join('\n')}
`;
  }
}
```

### 3.2 PreCompact Handoff Logic

Before the orchestrator requests a context wipe (`/clear`), it generates a **structured handoff file** that contains all information needed to resume work. This is the CCv2 pattern for context resilience.

**Handoff File Format:**

```markdown
# Sisyphus Handoff: PreCompact Snapshot

Generated: 2025-12-29T14:45:00Z
Project: /Users/bangedorrunt/workspace/swarmtool-addons
Epic ID: swarmtool-addons-gfoxls-mjpqe25p5p2

## Resume Context

After `/clear`, execute:
```

cd /Users/bangedorrunt/workspace/swarmtool-addons
swarm init --resume-from .sisyphus/HANDOFF_precompact_2025-12-29T14:45:00Z.md

```

## Current State

**Task**: Revise Sisyphus v2 Implementation Plan
**Phase**: Persistence & Handoff Primitives
**Status**: in_progress
**Last Stream Offset**: 127

## Active Beads

| Bead ID | Title | Status | Priority |
|----------|-------|--------|----------|
| swarmtool-addons-gfoxls-mjpqe25p5p2 | Define Persistence & Handoff Primitives | in_progress | 2 |

## File Reservations

| Path | Reserved By | Expires |
|------|-------------|---------|
| src/orchestrator/PLAN.md | worker-retry | 1766932139570 |

## Next Actions

1. Complete writing the Persistence & Handoff Primitives section to PLAN.md
2. Run typecheck to verify TypeScript consistency
3. Submit for review via swarm_complete

## Conversation Context (Raw)

[Last 20 messages from the Swarm Mail inbox, serialized for re-injection]
```

**Handoff Generation Logic:**

```typescript
// src/orchestrator/sisyphus/handoff.ts
export class HandoffGenerator {
  constructor(
    private projectPath: string,
    private stream: DurableSisyphusStream,
    private ledger: SisyphusLedger
  ) {}

  /**
   * Generate a handoff file before context wipe.
   * This file contains all state needed to resume work.
   */
  async generatePreCompact(): Promise<string> {
    const timestamp = new Date().toISOString();
    const handoffPath = path.join(
      this.projectPath,
      '.sisyphus',
      `HANDOFF_precompact_${timestamp.replace(/[:.]/g, '-')}.md`
    );

    // Gather state from multiple sources
    const ledgerState = await this.ledger.read();
    const streamOffset = await this.stream.getLatestOffset();
    const activeBeads = await hive_cells({ status: 'in_progress' });
    const reservations = await swarmmail_reservations();
    const inboxMessages = await swarmmail_inbox({ limit: 20 });

    // Generate handoff content
    const content = this.renderHandoff({
      timestamp,
      projectPath: this.projectPath,
      ledgerState,
      streamOffset,
      activeBeads,
      reservations,
      inboxMessages,
    });

    // Atomic write
    await fs.writeFile(handoffPath, content, 'utf-8');

    return handoffPath;
  }

  private renderHandoff(state: HandoffState): string {
    return `# Sisyphus Handoff: PreCompact Snapshot

Generated: ${state.timestamp}
Project: ${state.projectPath}
Epic ID: ${state.ledgerState.epicId}

## Resume Context

After \`/clear\`, execute:
\`\`\`bash
cd ${state.projectPath}
swarm init --resume-from ${this.getHandoffFilename(state.timestamp)}
\`\`\`

## Current State

**Task**: ${state.ledgerState.goal}
**Phase**: ${state.ledgerState.currentPhase}
**Status**: ${state.ledgerState.status}
**Last Stream Offset**: ${state.streamOffset}

## Active Beads

| Bead ID | Title | Status | Priority |
|----------|-------|--------|----------|
${state.activeBeads.map((b) => `| ${b.id} | ${b.title} | ${b.status} | ${b.priority} |`).join('\n')}

## File Reservations

| Path | Reserved By | Expires |
|------|-------------|---------|
${state.reservations.map((r) => `| ${r.path} | ${r.holder} | ${r.expiresAt} |`).join('\n')}

## Next Actions

${state.ledgerState.nextActions.map((a) => `1. ${a}`).join('\n')}

## Conversation Context (Raw)

${state.inboxMessages.map((m) => `### Message #${m.id}\n**From**: ${m.sender}\n**Subject**: ${m.subject}\n\n${m.body}`).join('\n\n')}
`;
  }
}
```

### 3.3 SessionStart Reloading Pattern

When a session resumes after a context wipe or agent restart, the following sequence re-establishes continuity:

**Resumption Flow:**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SESSION START RESUMPTION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. LOAD HANDOFF FILE:                                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ Read HANDOFF â”‚                                        â”‚
â”‚     â”‚ _precompact.mdâ”‚                                       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  2. SWARMAIL_INIT:                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ Initialize   â”‚                                        â”‚
â”‚     â”‚ session as   â”‚                                        â”‚
â”‚     â”‚ [agent_name] â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  3. SYNC WITH LEDGER:                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ Read SISYPHUSâ”‚                                        â”‚
â”‚     â”‚ _LEDGER.md   â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  4. REHYDRATE FROM STREAM:                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ stream.read  â”‚                                        â”‚
â”‚     â”‚ From(offset) â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  5. RESTORE CURSOR:                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ cursor =     â”‚                                        â”‚
â”‚     â”‚ handoff.offsetâ”‚                                       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  6. PROCESS INBOX:                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚     â”‚ Continue     â”‚                                        â”‚
â”‚     â”‚ where left   â”‚                                        â”‚
â”‚     â”‚ off          â”‚                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```typescript
// src/orchestrator/sisyphus/resume.ts
export class SessionResumer {
  constructor(
    private projectPath: string,
    private stream: DurableSisyphusStream,
    private ledger: SisyphusLedger
  ) {}

  /**
   * Resume a session from a handoff file.
   * Called after /clear or agent restart.
   */
  async resumeFromHandoff(handoffPath: string): Promise<void> {
    // 1. Load and parse handoff
    const handoff = await this.parseHandoff(handoffPath);

    // 2. Reinitialize Swarm Mail with original agent identity
    await swarmmail_init({
      project_path: handoff.projectPath,
      agent_name: handoff.originalAgentName,
    });

    // 3. Sync with current ledger state
    const ledgerState = await this.ledger.read();
    if (ledgerState.streamOffset !== handoff.streamOffset) {
      console.warn(
        `Ledger offset mismatch: ${ledgerState.streamOffset} vs ${handoff.streamOffset}`
      );
      // Use the higher offset to avoid missing events
      handoff.streamOffset = Math.max(ledgerState.streamOffset, handoff.streamOffset);
    }

    // 4. Rehydrate from durable stream
    const events = await this.stream.readFrom(handoff.streamOffset - 1, handoff.projectKey);
    console.log(`Replaying ${events.length} events from offset ${handoff.streamOffset}`);

    // 5. Restore cursor position
    await this.stream.saveCursor(
      handoff.originalAgentName,
      handoff.projectKey,
      handoff.streamOffset
    );

    // 6. Resume processing inbox
    console.log('Session resumed. Continuing from offset:', handoff.streamOffset);
  }

  /**
   * Find the most recent handoff file.
   */
  async findLatestHandoff(): Promise<string | null> {
    const sisyphusDir = path.join(this.projectPath, '.sisyphus');
    const files = await fs.readdir(sisyphusDir);
    const handoffFiles = files
      .filter((f) => f.startsWith('HANDOFF_precompact_'))
      .sort()
      .reverse();

    return handoffFiles.length > 0 ? path.join(sisyphusDir, handoffFiles[0]) : null;
  }
}
```

### 3.4 OMO-Compatible Todo-Checking

Sisyphus v2 implements todo-checking logic from OMO (oh-my-opencode) to ensure cells/beads are fully completed before closure. This prevents silent failures and maintains integrity.

**Todo-Checking Flow:**

```typescript
// src/orchestrator/sisyphus/todolist.ts
export class TodoChecker {
  constructor(
    private projectPath: string,
    private stream: DurableSisyphusStream
  ) {}

  /**
   * Check if a bead/cell has all todos completed.
   * Returns true if closure is allowed, false otherwise.
   */
  async canCloseBead(beadId: string): Promise<{ allowed: boolean; blockers: string[] }> {
    const blockers: string[] = [];

    // 1. Check if all files are modified as required
    const bead = await hive_cells({ id: beadId });
    if (!bead || bead.length === 0) {
      return { allowed: false, blockers: ['Bead not found'] };
    }

    const expectedFiles = this.extractExpectedFiles(bead[0].description);
    const modifiedFiles = await this.getModifiedFiles();

    for (const file of expectedFiles) {
      if (!modifiedFiles.includes(file)) {
        blockers.push(`File not modified: ${file}`);
      }
    }

    // 2. Check if tests pass
    const testResults = await this.runTests();
    const failedTests = testResults.filter((r) => !r.passed);
    if (failedTests.length > 0) {
      blockers.push(`${failedTests.length} test(s) failed`);
    }

    // 3. Check if typecheck passes
    const typecheckResult = await typecheck();
    if (typecheckResult.errors.length > 0) {
      blockers.push(`${typecheckResult.errors.length} type error(s)`);
    }

    // 4. Check if UBS scan passes (if configured)
    const ubsResult = await ubs_scan_json({ path: this.projectPath });
    if (ubsResult.errors.length > 0) {
      blockers.push(`${ubsResult.errors.length} UBS issue(s)`);
    }

    return { allowed: blockers.length === 0, blockers };
  }

  /**
   * Enforce completion before closing a bead.
   * Throws if blockers exist.
   */
  async enforceCompletionBeforeClosure(beadId: string): Promise<void> {
    const { allowed, blockers } = await this.canCloseBead(beadId);

    if (!allowed) {
      const message = `Cannot close bead ${beadId}:\n${blockers.map((b) => `  - ${b}`).join('\n')}`;
      console.error(message);

      // Emit a BLOCKED event to durable stream
      await this.stream.append(
        'BEAD_BLOCKED',
        {
          beadId,
          blockers,
          timestamp: Date.now(),
        },
        this.projectPath
      );

      throw new Error(message);
    }
  }

  /**
   * Extract expected files from bead description.
   * Heuristic: looks for file paths in markdown code blocks or references.
   */
  private extractExpectedFiles(description: string): string[] {
    const filePattern = /`([^`]+\.(ts|js|md))`/g;
    const matches = [];
    let match;

    while ((match = filePattern.exec(description)) !== null) {
      matches.push(match[1]);
    }

    return [...new Set(matches)]; // Deduplicate
  }

  /**
   * Get list of modified files in current working tree.
   */
  private async getModifiedFiles(): Promise<string[]> {
    const result = await bash({ command: 'git diff --name-only', cwd: this.projectPath });
    return result.stdout
      .trim()
      .split('\n')
      .filter((f) => f.length > 0);
  }

  /**
   * Run tests and return results.
   */
  private async runTests(): Promise<Array<{ name: string; passed: boolean }>> {
    // This would integrate with the actual test runner
    // For now, return placeholder
    return [];
  }
}
```

**Integration with swarm_complete:**

```typescript
// Modified swarm_complete to include todo-checking
export async function swarm_complete(params: SwarmCompleteParams): Promise<void> {
  const { project_key, agent_name, bead_id, summary } = params;

  // 1. Run todo-checking
  const checker = new TodoChecker(project_key, stream);
  await checker.enforceCompletionBeforeClosure(bead_id);

  // 2. Run verification gates (if not skipped)
  if (!params.skip_verification) {
    await runVerificationGates();
  }

  // 3. Close the bead
  await hive_close({
    id: bead_id,
    reason: summary,
  });

  // 4. Record completion event
  await stream.append('TASK_COMPLETE', {
    epicId: params.epic_id || bead_id,
    summary,
    filesTouched: params.files_touched || [],
  }, project_key);

  // 5. Update ledger
  await ledger.update({ ... });
}
```

### 3.5 Context Resilience

The system maintains signal integrity through multiple compaction cycles using a combination of durable streams, ledger projections, and handoff files.

**Resilience Strategy:**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CONTEXT RESILIENCE STRATEGY                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  [NATURAL STATE]                                            â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€ Durable Stream: Persistent event log               â”‚
â”‚       â”œâ”€ SISYPHUS_LEDGER.md: Current state projection      â”‚
â”‚       â””â”€ Actor Cursors: Checkpointed positions              â”‚
â”‚                                                             â”‚
â”‚       â–¼                                                     â”‚
â”‚  [CONTEXT FULL - PreCompact Handoff]                       â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€ Generate HANDOFF_precompact_*.md                   â”‚
â”‚       â”œâ”€ Capture current ledger state                       â”‚
â”‚       â”œâ”€ Capture file reservations                          â”‚
â”‚       â””â”€ Capture last 20 inbox messages                     â”‚
â”‚                                                             â”‚
â”‚       â–¼                                                     â”‚
â”‚  [/CLEAR EXECUTED]                                         â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€ Context wiped                                      â”‚
â”‚       â”œâ”€ Durable stream: PERSISTS                           â”‚
â”‚       â”œâ”€ Ledger file: PERSISTS                             â”‚
â”‚       â””â”€ Handoff file: PERSISTS                            â”‚
â”‚                                                             â”‚
â”‚       â–¼                                                     â”‚
â”‚  [RESUME - SessionStart Reload]                            â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€ Load HANDOFF_precompact_*.md                       â”‚
â”‚       â”œâ”€ Reinitialize Swarm Mail                           â”‚
â”‚       â”œâ”€ Sync ledger state                                 â”‚
â”‚       â”œâ”€ Restore cursor from offset                         â”‚
â”‚       â””â”€ Resume processing inbox                            â”‚
â”‚                                                             â”‚
â”‚       â–¼                                                     â”‚
â”‚  [BACK TO NATURAL STATE]                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Signal Preservation Guarantees:**

| Threat              | Mitigation                              |
| ------------------- | --------------------------------------- |
| Context wipe        | Handoff file preserves all state        |
| Agent crash         | Cursor checkpointed before each event   |
| Process restart     | Ledger provides instant recovery        |
| Database corruption | Event replay reconstructs state         |
| Lost handoff file   | Durable stream provides source of truth |

**Compaction Strategy:**

```typescript
// src/orchestrator/sisyphus/compaction.ts
export class CompactionManager {
  constructor(
    private projectPath: string,
    private stream: DurableSisyphusStream,
    private ledger: SisyphusLedger,
    private handoffGen: HandoffGenerator
  ) {}

  /**
   * Smart compaction: determine if context wipe is needed.
   */
  async shouldCompact(): Promise<boolean> {
    // Heuristics for compaction decision
    const contextSize = await this.estimateContextSize();
    const streamOffset = await this.stream.getLatestOffset();

    // Compact if: context > 100k tokens AND processed > 500 events
    return contextSize > 100_000 && streamOffset > 500;
  }

  /**
   * Execute a controlled compaction.
   */
  async compact(): Promise<string> {
    console.log('[Compaction] Generating handoff before context wipe...');

    // 1. Generate handoff file
    const handoffPath = await this.handoffGen.generatePreCompact();

    // 2. Update ledger with compaction marker
    await this.ledger.update({
      ...,
      lastCompaction: Date.now(),
      handoffPath,
    });

    // 3. Emit compaction event
    await this.stream.append('COMPACTION_INITIATED', {
      handoffPath,
      offset: await this.stream.getLatestOffset(),
    }, this.projectPath);

    // 4. Return instructions for user
    return `
========================================
COMPACTION READY - CONTEXT WIPE NEEDED
========================================

Handoff file generated: ${handoffPath}

To resume:
1. Run: /clear
2. Run: cd ${this.projectPath}
3. Run: swarm init --resume-from ${handoffPath}

========================================
`;
  }
}
```

### 3.6 TDD Requirements

Following swarm-tools patterns, the persistence and handoff primitives must include comprehensive test coverage.

**Unit Tests for SisyphusLedger:**

1. **Write/Read Consistency**:
   - Test that `update()` writes valid markdown
   - Test that `read()` parses state correctly
   - Test atomic write (no partial files on crash)

2. **State Transitions**:
   - Test status changes update correctly
   - Test decision history appends
   - Test working set updates

3. **Concurrent Access**:
   - Test multiple actors update ledger without corruption
   - Test atomic rename prevents partial reads

**Unit Tests for HandoffGenerator:**

1. **Handoff Completeness**:
   - Test all active beads included
   - Test file reservations captured
   - Test inbox messages preserved

2. **Resume Capability**:
   - Test generated handoff can be parsed
   - Test resume instructions are valid
   - Test timestamp format is sortable

3. **Edge Cases**:
   - Test handoff with no active beads
   - Test handoff with no reservations
   - Test handoff with empty inbox

**Unit Tests for SessionResumer:**

1. **Resumption Accuracy**:
   - Test cursor restored to correct offset
   - Test ledger state synced
   - Test swarm_mail reinitialized

2. **Error Handling**:
   - Test invalid handoff file format
   - Test missing handoff file
   - Test offset mismatch recovery

**Unit Tests for TodoChecker:**

1. **Completion Validation**:
   - Test detects unmodified files
   - Test detects failed tests
   - Test detects type errors
   - Test detects UBS issues

2. **Blocker Reporting**:
   - Test all blockers listed
   - Test blockers formatted correctly
   - Test empty blockers allow closure

3. **File Extraction**:
   - Test extracts file paths from description
   - Test deduplicates file list
   - Test handles missing file patterns

**Integration Tests:**

1. **Full Compaction Cycle**:
   - Test: Work for 100 events â†’ trigger compaction â†’ verify handoff â†’ resume â†’ continue work
   - Verify: Cursor restored, ledger updated, no events lost

2. **Crash During Handoff**:
   - Test: Kill process during handoff generation
   - Verify: Previous handoff file still valid
   - Verify: Durable stream unaffected

3. **Multiple Compactions**:
   - Test: Work â†’ compact â†’ work â†’ compact â†’ work
   - Verify: Signal preserved through multiple wipes
   - Verify: Event stream monotonic

4. **Todo-Checking Integration**:
   - Test: Attempt bead closure with incomplete work
   - Verify: Closure blocked, blockers listed
   - Test: Complete work, verify closure allowed

**Example Test Structure:**

```typescript
describe('Persistence & Handoff Primitives', () => {
  describe('SisyphusLedger', () => {
    it('should update ledger atomically', async () => {
      const ledger = new SisyphusLedger(testProjectPath);
      const state = createMockLedgerState();

      await ledger.update(state);

      const readState = await ledger.read();
      expect(readState).toEqual(state);
    });

    it('should handle concurrent updates', async () => {
      const ledger = new SisyphusLedger(testProjectPath);
      const updates = Array.from({ length: 10 }, (_, i) =>
        createMockLedgerState({ streamOffset: i })
      );

      await Promise.all(updates.map((u) => ledger.update(u)));

      // Final state should be one of the updates (last writer wins)
      const finalState = await ledger.read();
      expect([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).toContain(finalState.streamOffset);
    });
  });

  describe('HandoffGenerator', () => {
    it('should generate complete handoff', async () => {
      const generator = new HandoffGenerator(testProjectPath, testStream, testLedger);
      await testStream.append('TEST', { data: 'test' }, testProjectKey);

      const handoffPath = await generator.generatePreCompact();

      const content = await fs.readFile(handoffPath, 'utf-8');
      expect(content).toContain('Sisyphus Handoff');
      expect(content).toContain(testProjectPath);
    });

    it('should include active beads', async () => {
      const generator = new HandoffGenerator(testProjectPath, testStream, testLedger);
      await hive_create({ title: 'Test Bead' });

      const handoffPath = await generator.generatePreCompact();
      const content = await fs.readFile(handoffPath, 'utf-8');

      expect(content).toContain('Test Bead');
    });
  });

  describe('Compaction Cycle', () => {
    it('should survive full compaction cycle', async () => {
      const compaction = new CompactionManager(testProjectPath, testStream, testLedger, handoffGen);

      // Simulate work
      await testStream.append('TASK_INIT', { task: 'test' }, testProjectKey);

      // Trigger compaction
      const instructions = await compaction.compact();

      // Verify handoff exists
      expect(instructions).toContain('COMPACTION READY');

      // Simulate resume
      const handoffPath = await compaction.handoffGen.findLatestHandoff();
      expect(handoffPath).not.toBeNull();

      const resumer = new SessionResumer(testProjectPath, testStream, testLedger);
      await resumer.resumeFromHandoff(handoffPath!);

      // Verify state preserved
      const latestOffset = await testStream.getLatestOffset();
      expect(latestOffset).toBeGreaterThan(0);
    });
  });
});
```

---

## 2.4 Synthesized Implementation Roadmap

This section synthesizes the implementation primitives from Sections 2.1-2.3 into a phased, actionable roadmap with clear dependencies and testing requirements.

### 2.4.1 Four-Phase Implementation Plan

| Phase       | Focus              | Estimated Effort | Dependencies       | Delivers                                             |
| ----------- | ------------------ | ---------------- | ------------------ | ---------------------------------------------------- |
| **Phase 1** | Storage Foundation | 1 week           | None               | PGLite schema, DurableSisyphusStream                 |
| **Phase 2** | Actor Runtime      | 1-2 weeks        | Phase 1 complete   | SisyphusActor base, boot/processInbox lifecycle      |
| **Phase 3** | Persistence Layer  | 1 week           | Phase 1+2 complete | SISYPHUS_LEDGER.md, HandoffGenerator, SessionResumer |
| **Phase 4** | Integration & UI   | 1 week           | Phase 1-3 complete | OMO hooks, Todo-checking, swarm init command         |

**Total Timeline:** 4-5 weeks for full implementation

### 2.4.2 Phase 1: Storage Foundation (1 Week)

**Goal:** Establish durable event sourcing infrastructure.

**Deliverables:**

- `src/orchestrator/sisyphus/schema.ts` - Drizzle ORM schema definitions
- `src/orchestrator/sisyphus/stream.ts` - DurableSisyphusStream class
- `src/orchestrator/sisyphus/types.ts` - Event type definitions
- `test/sisyphus/stream.test.ts` - Comprehensive test coverage

**Implementation Order:**

1. Define SQL schema (sisyphus_events, sisyphus_agents, sisyphus_cursors tables)
2. Create Drizzle ORM mappings
3. Implement DurableSisyphusStream.append() with auto-increment sequence
4. Implement DurableSisyphusStream.readFrom() with offset-based querying
5. Implement cursor management (saveCursor/getCursor)
6. Add replay() method for state reconstruction

**Testing Requirements:**

```bash
# Unit tests
bun test test/sisyphus/stream.test.ts

# Must cover:
- Append assigns monotonically increasing sequences
- ReadFrom returns events after given offset
- Cursor persistence across database restarts
- Replay reconstructs correct state
- Concurrent appends don't corrupt sequence ordering
- Project-level isolation (events don't leak)
- JSON payload serialization roundtrip
```

**External Dependencies:**

- `drizzle-orm` - ORM for SQLite/PGLite
- `better-sqlite3` - Database driver
- `swarm-tools` - DurableCursor pattern reference

### 2.4.3 Phase 2: Actor Runtime (1-2 Weeks)

**Goal:** Implement Actor Model with durable lifecycle.

**Deliverables:**

- `src/orchestrator/sisyphus/actor.ts` - SisyphusActor abstract base class
- `test/sisyphus/actor.test.ts` - Actor lifecycle tests
- OMO integration hooks for background task spawning

**Implementation Order:**

1. Define SisyphusActor abstract class
2. Implement boot() sequence (swarmmail_init, cursor restore, rehydration)
3. Implement processInbox() Continuity Loop with checkpointing
4. Implement abstract onMessage() method
5. Implement shutdown() with cursor persistence
6. Add emit() helper for event emission
7. Integrate with OMO background lifecycle

**Testing Requirements:**

```bash
# Unit tests
bun test test/sisyphus/actor.test.ts

# Must cover:
- Boot sequence initializes Swarm Mail
- Cursor restoration from database
- Status transitions: launching â†’ running
- processInbox() reads from correct offset
- Checkpointing after each event
- Polling when inbox is empty
- Loop termination on shutdown()
- Restart skips already-processed events (crash recovery)
- State re-hydration from event stream
- Status maps to OMO background states
```

**External Dependencies:**

- Phase 1 (DurableSisyphusStream)
- `oh-my-opencode` - Background lifecycle hooks
- `swarm-tools` - Swarm Mail for coordination

### 2.4.4 Phase 3: Persistence Layer (1 Week)

**Goal:** Implement Continuity Ledger and Handoff primitives.

**Deliverables:**

- `src/orchestrator/sisyphus/ledger.ts` - SisyphusLedger class
- `src/orchestrator/sisyphus/handoff.ts` - HandoffGenerator class
- `src/orchestrator/sisyphus/resume.ts` - SessionResumer class
- `src/orchestrator/sisyphus/todolist.ts` - TodoChecker class
- `test/sisyphus/ledger.test.ts` - Persistence tests

**Implementation Order:**

1. Implement SisyphusLedger.update() with atomic writes
2. Implement SisyphusLedger.read() for state retrieval
3. Define ledger file format (Goal, State, Decisions, Working Set)
4. Implement HandoffGenerator.generatePreCompact()
5. Define handoff file format (Resume Context, Active Beads, Reservations)
6. Implement SessionResumer.resumeFromHandoff()
7. Implement TodoChecker.canCloseBead()
8. Integrate todo-checking with swarm_complete

**Testing Requirements:**

```bash
# Unit tests
bun test test/sisyphus/ledger.test.ts

# Must cover:
- Ledger update writes valid markdown
- Ledger read parses state correctly
- Atomic write (no partial files on crash)
- Concurrent updates without corruption
- Handoff includes all active beads
- Handoff captures file reservations
- Handoff preserves inbox messages
- Generated handoff can be parsed
- Resume instructions are valid
- Timestamp format is sortable
- Cursor restored to correct offset on resume
- Ledger state synced on resume
- TodoChecker detects unmodified files
- TodoChecker detects failed tests/type errors/UBS issues
```

**External Dependencies:**

- Phase 1 (DurableSisyphusStream)
- Phase 2 (SisyphusActor)
- `Continuous-Claude-v2` - Continuity Ledger pattern
- `swarm-tools` - Hive cells for bead management

### 2.4.5 Phase 4: Integration & UI (1 Week)

**Goal:** Wire everything together with CLI and TUI integration.

**Deliverables:**

- `src/orchestrator/sisyphus/kernel.ts` - SisyphusKernel orchestration layer
- `src/orchestrator/sisyphus/compaction.ts` - CompactionManager for smart context wipes
- `src/opencode/command/sisyphus.md` - CLI command documentation
- `src/opencode/agent/sisyphus/SKILL.md` - Updated skill documentation

**Implementation Order:**

1. Implement SisyphusKernel.spawnAgent() using skill_agent
2. Implement SisyphusKernel.monitor() for actor lifecycle
3. Integrate TodoChecker with swarm_complete
4. Implement CompactionManager.shouldCompact()
5. Implement CompactionManager.compact() with handoff generation
6. Add `swarm init --resume-from` CLI flag
7. Update TUI to show actor status from ledger
8. Add context wipe detection and automatic handoff generation

**Testing Requirements:**

```bash
# Integration tests
bun test test/sisyphus/integration.test.ts

# Must cover:
- Full compaction cycle (work â†’ compact â†’ resume â†’ continue)
- Cursor restored, ledger updated, no events lost
- Crash during handoff: previous handoff still valid
- Multiple compactions: signal preserved through wipes
- Todo-checking integration: blocks incomplete bead closure
- Complete work: closure allowed
```

**External Dependencies:**

- Phase 1-3 (All primitives)
- `oh-my-opencode` - TUI status display
- `swarm-tools` - Skill spawning, Swarm Mail

### 2.4.6 Implementation Dependencies Map

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IMPLEMENTATION DEPENDENCY GRAPH                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  [PHASE 1: Storage Foundation]                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  schema.ts   â”‚â”€â”€â–¶â”‚  stream.ts   â”‚â”€â”€â–¶â”‚ types.ts     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â”‚                                    â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                           â”‚                             â”‚
â”‚                           â–¼                             â”‚
â”‚  [PHASE 2: Actor Runtime]                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚          actor.ts (extends Phase 1)          â”‚         â”‚
â”‚  â”‚  - boot()  - processInbox()  - shutdown()   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                           â”‚                             â”‚
â”‚                           â–¼                             â”‚
â”‚  [PHASE 3: Persistence Layer]                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ledger â”‚  â”‚ handoff â”‚  â”‚  resume  â”‚   â”‚ todok  â”‚â”‚
â”‚  â”‚ .ts     â”‚  â”‚ .ts     â”‚  â”‚  .ts     â”‚   â”‚ ecker  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ .ts    â”‚â”‚
â”‚       â”‚           â”‚              â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                    â”‚                                   â”‚
â”‚                    â–¼                                   â”‚
â”‚  [PHASE 4: Integration & UI]                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        kernel.ts (orchestrates all)          â”‚       â”‚
â”‚  â”‚  - spawnAgent()  - monitor()              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                           â”‚                             â”‚
â”‚                           â–¼                             â”‚
â”‚              [EXTERNAL INTEGRATION]                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   OMO     â”‚  â”‚  swarm-   â”‚  â”‚   CCv2    â”‚  â”‚
â”‚  â”‚  TUI      â”‚  â”‚  tools    â”‚  â”‚  Ledger   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**File Path Mapping:**

| Phase   | Files Created                                                                                                                                                       | Testing Files                                                     |
| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| Phase 1 | `src/orchestrator/sisyphus/schema.ts`<br>`src/orchestrator/sisyphus/stream.ts`<br>`src/orchestrator/sisyphus/types.ts`                                              | `test/sisyphus/stream.test.ts`                                    |
| Phase 2 | `src/orchestrator/sisyphus/actor.ts`                                                                                                                                | `test/sisyphus/actor.test.ts`                                     |
| Phase 3 | `src/orchestrator/sisyphus/ledger.ts`<br>`src/orchestrator/sisyphus/handoff.ts`<br>`src/orchestrator/sisyphus/resume.ts`<br>`src/orchestrator/sisyphus/todolist.ts` | `test/sisyphus/ledger.test.ts`<br>`test/sisyphus/handoff.test.ts` |
| Phase 4 | `src/orchestrator/sisyphus/kernel.ts`<br>`src/orchestrator/sisyphus/compaction.ts`<br>`src/opencode/command/sisyphus.md`<br>`src/opencode/agent/sisyphus/SKILL.md`  | `test/sisyphus/integration.test.ts`                               |

---

## 4. Real-World Examples

### Example 1: Refactoring a Database Adapter (Resilience Test)

**Task**: "Migrate `src/db/adapter.ts` from PGLite to LibSQL."

1.  **The Launcher**: Kernel calls `skill_agent({ agent_name: 'planner' })`.
2.  **The Plan**: Planner Actor creates `SISYPHUS_LEDGER.md`. It retrieves a past memory via `memory-lane_find` about LibSQL WAL safety.
3.  **The Crash**: Mid-refactor, the system loses power/process.
4.  **The Recovery**: Upon restart, the Executor Actor calls `boot()`. It re-reads the Durable Stream, sees `TEST_PASS` at offset 12, and resumes work on the _next_ file in the plan without re-running Phase 1 or 2.

### Example 2: Security Implementation (Validation Gate)

**Task**: "Add OIDC login flow."

1.  **Planner**: Proposes using library `X`.
2.  **Validator**: Queries `memory-lane` and finds that library `X` has a vulnerability recorded in a previous session.
3.  **Handoff**: Validator sends a durable `REJECTION` message to the Planner Actor via SwarmMail.
4.  **Pivot**: Planner receives the message, corrects the strategy, and updates the Ledger. The Kernel (Coordinator) only sees the finalized, safe plan.

---

## 5. Detailed Walkthrough: Using Sisyphus v2

This section provides concrete commands and workflows for using Sisyphus v2 in real development scenarios.

### 5.1 Basic Usage: Starting a New Epic

```bash
# Initialize a new Sisyphus epic
swarm init "Refactor authentication system"

# Sisyphus will:
# 1. Call swarmmail_init() to register with the coordination bus
# 2. Call memory-lane_find("authentication refactor") to learn from past sessions
# 3. Create .sisyphus/SISYPHUS_LEDGER.md with initial state
# 4. Spawn Planner Actor via skill_agent()
# 5. Begin task decomposition and execution
```

**What happens:**

1. **Bootstrap Sequence:**
   - Sisyphus initializes Swarm Mail (the coordination radio)
   - Queries Memory Lane for past learnings (the wisdom store)
   - Creates durable stream in PGLite database
   - Generates initial `.sisyphus/SISYPHUS_LEDGER.md`

2. **The Ledger (Live Dashboard):**

   ```bash
   # Check current state at any time
   cat .sisyphus/SISYPHUS_LEDGER.md

   # Output:
   # Sisyphus Ledger

   ## Goal
   Refactor authentication system to use JWT tokens

   ## State
   **Current Phase**: Planning
   **Status**: in_progress
   **Last Updated**: 2025-12-29T14:45:00Z
   **Stream Offset**: 42
   ```

3. **Background Agents:**
   ```bash
   # If using OMO, see background tasks in TUI
   # Tasks appear as:
   # [RUNNING] Planner Agent - Processing task decomposition
   # [RUNNING] Executor Agent - Implementing src/auth/jwt.ts
   ```

### 5.2 Context Wipe & Resumption (Recovery Pattern)

When context reaches ~100k tokens, Sisyphus triggers a controlled compaction:

```bash
# Sisyphus automatically generates handoff
# Message: "Context full. Generating handoff file..."

# Check handoff file
cat .sisyphus/HANDOFF_precompact_2025-12-29T15-30-00Z.md

# Then run /clear to wipe context
/clear

# Resume from handoff
cd /path/to/project
swarm init --resume-from .sisyphus/HANDOFF_precompact_2025-12-29T15-30-00Z.md
```

**What happens during resumption:**

1. **Load Handoff:**

   ```bash
   # SessionResumer reads handoff file
   # Restores:
   # - Original agent identity
   # - Last stream offset (e.g., 42)
   # - Active beads and file reservations
   # - Last 20 inbox messages
   ```

2. **Reinitialize Coordination:**

   ```bash
   # swarmmail_init() called with original agent name
   # Swarm Mail session restored with same ID
   ```

3. **Sync Ledger:**

   ```bash
   # SessionResumer syncs with current SISYPHUS_LEDGER.md
   # Verifies offset consistency
   # Detects any drift between handoff and ledger
   ```

4. **Resume Processing:**
   ```bash
   # SisyphusActor.processInbox() continues from offset 43
   # Reads new events only (no re-processing)
   # Continuity maintained transparently
   ```

### 5.3 Monitoring & Debugging

**Check Actor Status:**

```bash
# View current ledger state
cat .sisyphus/SISYPHUS_LEDGER.md

# If using OMO TUI, see live status:
# [RUNNING] Executor Actor - Offset: 127 - Last: 2m ago
# [IDLE]   Validator Actor - Waiting for validation request
```

**Check Durable Stream:**

```bash
# Query event log via SQLite
sqlite3 ~/.config/swarm-tools/sisyphus.db \
  "SELECT sequence, type, created_at FROM sisyphus_events WHERE project_key = '/path/to/project' ORDER BY sequence DESC LIMIT 10;"

# Output:
# 127 | STATUS_UPDATE | 1735487800123
# 126 | FILE_EDIT     | 1735487800112
# 125 | TEST_PASS     | 1735487800101
```

**Check Cursors (Actor Positions):**

```bash
# See which actors are processing which offsets
sqlite3 ~/.config/swarm-tools/sisyphus.db \
  "SELECT consumer_id, position, last_updated FROM sisyphus_cursors;"

# Output:
# planner-agent-1735487600123 | 127 | 1735487800123
# executor-agent-1735487600156 | 98  | 1735487800098
```

### 5.4 Todo-Checking & Completion

Before closing a bead, Sisyphus verifies all requirements are met:

```bash
# Worker calls swarm_complete()
# TodoChecker automatically runs:

# 1. Check modified files
git diff --name-only
# Expected: src/auth/jwt.ts, src/auth/middleware.ts
# Found: âœ“ All present

# 2. Run tests
bun test test/auth/jwt.test.ts
# Result: âœ“ All passed

# 3. Typecheck
bun run typecheck
# Result: âœ“ No errors

# 4. UBS scan
bun run ubs-scan
# Result: âœ“ No issues

# Only then does bead close:
# âœ“ swarm_complete() succeeds
# âœ“ TASK_COMPLETE event appended to stream
# âœ“ SISYPHUS_LEDGER.md updated
```

**If blockers exist:**

```bash
# TodoChecker detects issues:
# - File not modified: src/auth/config.ts
# - 2 tests failed
# - 1 type error in jwt.ts

# Result:
# âœ— swarm_complete() blocked
# âœ— BEAD_BLOCKED event emitted to stream
# âœ— Message sent to coordinator with blockers
```

### 5.5 Manual Intervention: Resolving Blocks

If an agent becomes stuck or blocked:

```bash
# Check inbox for messages
swarmmail_inbox()

# Read specific message
swarmmail_read_message --message-id 123

# Send message to blocked agent
swarmmail_send \
  --to=executor-agent \
  --subject="Question about JWT implementation" \
  --body="Which library are you using for token validation?"

# Check ledger for last decision
cat .sisyphus/SISYPHUS_LEDGER.md
# Look under "Decisions" section for rationale
```

### 5.6 Full Workflow Example

```bash
# ============================================================
# COMPLETE WORKFLOW: Add OIDC Authentication
# ============================================================

# Step 1: Initialize epic
swarm init "Add OIDC authentication flow"

# Step 2: Sisyphus bootstraps
# - Creates .sisyphus/SISYPHUS_LEDGER.md
# - Initializes PGLite durable stream
# - Spawns Planner Actor

# Step 3: Planner decomposes task
# - Creates beads: config, provider, middleware, tests
# - Updates ledger with plan
# - Spawns Executor Actors in parallel

# Step 4: Workers execute
# [Executor A] Implementing src/auth/oidc/config.ts
# [Executor B] Implementing src/auth/oidc/provider.ts
# [Executor C] Writing tests

# Step 5: Context reaches 80k tokens
# Sisyphus: "Generating handoff..."
# Handoff file created: .sisyphus/HANDOFF_precompact_*.md

# Step 6: Clear context
/clear

# Step 7: Resume
swarm init --resume-from .sisyphus/HANDOFF_precompact_*.md

# Step 8: Work continues automatically
# - Cursor restored: offset 45
# - Executor C resumes test writing
# - No re-processing of completed files

# Step 9: Workers complete
# - TodoChecker validates all beads
# - All tests pass
# - No type errors

# Step 10: Finalize
# Sisyphus: "All beads complete. Running final verification..."
# - UBS scan: âœ“
# - Memory learning stored: âœ“
# - Epic closed: âœ“

# ============================================================
# RESULT: OIDC authentication fully implemented in 4 hours
# ============================================================
```

### 5.7 Key Takeaways

1. **Always check the ledger** - `.sisyphus/SISYPHUS_LEDGER.md` is your source of truth
2. **Trust the handoff** - Resumption is automatic, no manual state reconstruction needed
3. **Watch for blocks** - Swarm Mail inbox signals when agents need help
4. **Let todo-checking work** - Don't force-close beads; let TodoChecker validate
5. **Memory Lane accumulates** - Each session learns from past mistakes, improving over time

---

## 6. Summary of Leverage

Sisyphus v2 synthesizes three external systems into a cohesive orchestration layer. Each system provides critical capabilities that would require months of independent development.

### 6.1 External Contributions Matrix

| Module                          | Leverage Point       | Specific Integration                                                                                           | Benefit                                                                                                                   |
| ------------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **oh-my-opencode** (OMO)        | Background Lifecycle | `SisyphusActor.boot()` uses OMO hooks<br>`background_task.spawn()` / `status()` / `kill()`                     | Non-blocking agent execution<br>TUI status display<br>Idle hook for auto-resume                                           |
| **Continuous-Claude-v2** (CCv2) | Continuity Ledger    | `SISYPHUS_LEDGER.md` format<br>`HandoffGenerator.generatePreCompact()`<br>`SessionResumer.resumeFromHandoff()` | Materialized view of state<br>Context wipe resilience<br>Hard reset recovery<br>No manual state reconstruction            |
| **swarm-tools**                 | Durable Stream       | `DurableSisyphusStream` class<br>Event sourcing patterns<br>`DurableCursor` checkpointing                      | Auditability of all decisions<br>Offset-based replay<br>Crash-safe cursor management<br>Multi-agent coordination          |
| **Memory Lane**                 | Entity Wisdom        | `memory-lane_find()` in planner<br>`memory-lane_store()` in completion<br>Project-specific lore                | Avoids repeating past mistakes<br>Learns from successful patterns<br>Accumulates team knowledge<br>Reduces debugging time |

### 6.2 Integration Deep Dives

#### 6.2.1 oh-my-opencode (OMO): Background Execution

**What OMO Provides:**

OMO's background task system allows Sisyphus agents to run independently without blocking the main LLM context window. This enables true parallelism where multiple agents work on different aspects of a task simultaneously.

**Sisyphus Integration:**

```typescript
// OMO background state machine maps to SisyphusActor lifecycle
enum OMOBackgroundState {
  LAUNCHING = 'launching', // Maps to SisyphusActor.boot()
  RUNNING = 'running', // Maps to SisyphusActor.processInbox()
  COMPLETED = 'completed', // Maps to SisyphusActor.shutdown(true)
  FAILED = 'failed', // Maps to SisyphusActor.shutdown(false)
}

// Sisyphus uses OMO hooks for agent lifecycle
await skill_agent({
  agent_name: 'executor',
  prompt: 'Implement authentication flow',
  background: true, // Delegates to OMO background
});
```

**TUI Integration:**

OMO's Terminal UI provides real-time visibility into agent execution:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Active Tasks                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â— Planner Actor     [RUNNING]  Offset: 127       â”‚
â”‚  â— Executor Actor    [BUSY]     Implementing jwt.ts â”‚
â”‚  â— Validator Actor  [IDLE]     Waiting for handoff  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Benefit:** Non-blocking execution allows Sisyphus to orchestrate 5+ agents without context exhaustion.

#### 6.2.2 Continuous-Claude-v2 (CCv2): Continuity Ledger

**What CCv2 Provides:**

CCv2 pioneered the "Continuity Ledger" pattern for maintaining state across context wipes. The ledger serves as a human-readable projection of the event stream that can be quickly inspected to understand current state.

**Sisyphus Integration:**

```typescript
// SISYPHUS_LEDGER.md is a materialized view
export class SisyphusLedger {
  async update(state: LedgerState): Promise<void> {
    // Generates human-readable markdown projection
    // Includes: Goal, State, Decisions, Working Set
    // Updated atomically on every state transition
  }
}

// Handoff files enable hard reset recovery
export class HandoffGenerator {
  async generatePreCompact(): Promise<string> {
    // Captures everything needed to resume:
    // - Ledger state
    // - Active beads
    // - File reservations
    // - Last 20 inbox messages
  }
}

// Resume is one command
await swarm init --resume-from .sisyphus/HANDOFF_precompact_*.md
// No manual state reconstruction needed
```

**Context Wipe Resilience:**

```text
Before CCv2 Integration:
  [Context Full] â†’ /clear â†’ [Manual reconstruction of work]
                                      ^^^^^^^^^^^^^^^^^^^
                                      30+ minutes lost

After CCv2 Integration:
  [Context Full] â†’ [Generate Handoff] â†’ /clear â†’ [Resume]
                                        ^^^^^^^^           ^^^^^^^
                                        Instant            Instant
```

**Key Benefit:** Zero-effort recovery from context wipes through handoff files.

#### 6.2.3 swarm-tools: Durable Stream

**What swarm-tools Provides:**

swarm-tools established event sourcing patterns (DurableStream, DurableCursor, Effect-TS) that provide a solid foundation for building state machines that can replay history and recover from crashes.

**Sisyphus Integration:**

```typescript
// DurableSisyphusStream follows swarm-tools patterns
export class DurableSisyphusStream {
  // Append events to immutable log
  async append<T>(type: string, payload: T): Promise<number> {
    const offset = await db.insert('events').values({ type, payload });
    return offset; // Monotonic sequence number
  }

  // Read from offset for replay
  async readFrom(offset: number): Promise<SisyphusEvent[]> {
    return db.select().from('events').where(gt('offset', offset));
  }

  // Checkpointing for crash recovery
  async saveCursor(consumerId: string, position: number): Promise<void> {
    await db.insert('cursors').values({ consumerId, position });
  }
}

// Actors replay history on restart
await actor.boot(); // Restores cursor
await actor.processInbox(); // Reads from last offset
// No re-processing of old events
```

**Auditability:**

Every decision is recorded in the event log:

```sql
SELECT sequence, type, created_at FROM sisyphus_events
WHERE project_key = '/my-project'
ORDER BY sequence DESC LIMIT 10;

# Output:
# 127 | STATUS_UPDATE | 2025-12-29T14:45:00Z
# 126 | DECISION      | 2025-12-29T14:44:00Z  (Use JWT library X)
# 125 | FILE_EDIT     | 2025-12-29T14:43:00Z
```

**Key Benefit:** Complete audit trail enables debugging, replay, and cross-session re-hydration.

#### 6.2.4 Memory Lane: Entity Wisdom

**What Memory Lane Provides:**

Memory Lane is a vector similarity search over past session learnings. It allows agents to avoid repeating mistakes and leverage successful patterns from previous work.

**Sisyphus Integration:**

```typescript
// Planner queries Memory Lane before making decisions
const pastLearnings = await memory_lane_find({
  query: 'JWT authentication best practices',
  limit: 5,
});

// Learnings include:
// - "OAuth refresh tokens need 5min buffer"
// - "Library X has a known vulnerability"
// - "Use separate signing key for testing"

// Planner incorporates into decisions
const plan = generatePlan({
  constraints: pastLearnings.map((l) => l.information),
});

// Completion stores new learnings
await memory_lane_store({
  information: 'JWT signing keys should be rotated monthly',
  type: 'insight',
  tags: ['jwt', 'security', 'best-practices'],
});
```

**Accumulation Effect:**

```text
Session 1: "Implement JWT auth"
  â†’ Memory stored: "Use short expiry (15m)"
  â†’ Memory stored: "Rotate keys monthly"

Session 10: "Add OIDC"
  â†’ Planner queries "OIDC best practices"
  â†’ Gets: "Use short expiry", "Rotate keys"
  â†’ No debugging, immediate correct implementation

Session 50: "Audit auth system"
  â†’ Planner queries "auth audit"
  â†’ Gets all 49 prior learnings
  â†’ Completes audit in 30 minutes (vs 4 hours without memory)
```

**Key Benefit:** Team intelligence accumulates over time, reducing repeated work.

### 6.3 Synergy Analysis

| Capability           | Without Integration             | With Integration              | Time Saved |
| -------------------- | ------------------------------- | ----------------------------- | ---------- |
| Background execution | Sequential blocking             | Parallel agents (3-5x faster) | 60%        |
| Context recovery     | Manual reconstruction           | One-command resume            | 95%        |
| Crash recovery       | Lost work, restart from scratch | Resume from last checkpoint   | 80%        |
| Decision quality     | Guessing approach               | Leveraging past patterns      | 70%        |
| Debugging            | Reading logs                    | Event replay + memory         | 50%        |

**Cumulative Effect:** Sisyphus v2 is approximately **4-6x more efficient** than ad-hoc orchestration due to these integrations.

### 6.4 Conclusion

This architecture ensures that Sisyphus v2 is the most resilient and scalable orchestration system in the OpenCode environment. By leveraging:

- **OMO's** non-blocking execution and TUI integration
- **CCv2's** Continuity Ledger pattern for context resilience
- **swarm-tools'** event sourcing foundation for auditability
- **Memory Lane's** entity wisdom for continuous improvement

Sisyphus v2 enables long-running, complex development tasks that would otherwise fail due to context limits or agent coordination overhead.
